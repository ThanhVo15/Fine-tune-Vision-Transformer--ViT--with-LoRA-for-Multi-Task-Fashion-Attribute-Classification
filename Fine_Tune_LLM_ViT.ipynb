{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. https://ai.plainenglish.io/understanding-low-rank-adaptation-lora-for-efficient-fine-tuning-of-large-language-models-082d223bb6db\n",
        "2. https://medium.com/@manindersingh120996/practical-guide-to-fine-tune-llms-with-lora-c835a99d7593\n",
        "3. https://medium.com/@imabhi1216/fine-tuning-a-vision-transformer-vit-model-with-a-custom-dataset-37840e4e9268"
      ],
      "metadata": {
        "id": "9yL8nTH9v-Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Pip Install / Load Libraries / Load Dataset form Hugging face"
      ],
      "metadata": {
        "id": "bAuhBBj6wClV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets torchvision accelerate evaluate"
      ],
      "metadata": {
        "id": "e1pT6JggwBNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVM_UoGym1Sc"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "# L·∫•y th√¥ng tin RAM (s·ªë byte kh·∫£ d·ª•ng)\n",
        "available_ram = psutil.virtual_memory().available\n",
        "# L·∫•y s·ªë l∆∞·ª£ng l√µi CPU (logical)\n",
        "cpu_count = psutil.cpu_count(logical=True)\n",
        "\n",
        "print(f\"Available RAM: {available_ram / (1024**3):.2f} GB\")\n",
        "print(f\"CPU count: {cpu_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_from_disk\n",
        "\n",
        "from datasets import load_dataset, Features, Value\n",
        "from transformers import ViTFeatureExtractor\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "cTgGYeEbwHn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict  = load_dataset(\"ashraq/fashion-product-images-small\")\n",
        "dataset_dict"
      ],
      "metadata": {
        "id": "ula_bbMDwK2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Basic Preprocessing and Understanding Data"
      ],
      "metadata": {
        "id": "U-Mbdrk5wMrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset_dict[\"train\"]\n",
        "train_dataset_org = dataset_dict[\"train\"]"
      ],
      "metadata": {
        "id": "0YhNUCGOwN5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shown_labels = set()\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Loop through the dataset and plot the first image of each label\n",
        "for i, sample in enumerate(train_dataset):\n",
        "    label = sample[\"masterCategory\"]  # l·∫•y nh√£n tr·ª±c ti·∫øp v√¨ n√≥ ƒë√£ l√† chu·ªói\n",
        "    if label not in shown_labels:\n",
        "        plt.subplot(1, len(set([s[\"masterCategory\"] for s in train_dataset])), len(shown_labels) + 1)\n",
        "        plt.imshow(sample[\"image\"])\n",
        "        plt.title(label)\n",
        "        plt.axis(\"off\")\n",
        "        shown_labels.add(label)\n",
        "        # N·∫øu ƒë√£ hi·ªÉn th·ªã h·∫øt c√°c nh√£n, d·ª´ng v√≤ng l·∫∑p\n",
        "        if len(shown_labels) == len(set([s[\"masterCategory\"] for s in train_dataset])):\n",
        "            break\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-i2odR9twRaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Add Random Price Column"
      ],
      "metadata": {
        "id": "e6HC2ZPTwZs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_price(masterCategory, subCategory, articleType, year, season, usage):\n",
        "    master_price_range = {\n",
        "        'Apparel': (10, 20), 'Accessories': (5, 100), 'Footwear': (20, 200),\n",
        "        'Personal Care': (5, 300), 'Free Items': (0, 5), 'Sporting Goods': (100, 250), 'Home': (100, 300)\n",
        "    }\n",
        "    sub_price_adjustment = {\n",
        "        'Topwear': (5, 50), 'Watches': (30, 150), 'Shoes': (20, 100),\n",
        "        'Bags': (10, 70), 'Fragrance': (5, 80), 'Jewellery': (10, 200)\n",
        "    }\n",
        "    article_price_range = {\n",
        "        'Shirts': (15, 70), 'Watches': (50, 300), 'Sports Shoes': (40, 200),\n",
        "        'Sunglasses': (30, 150), 'Perfumes': (25, 120)\n",
        "    }\n",
        "\n",
        "    # L·∫•y kho·∫£ng gi√° t·ª´ masterCategory\n",
        "    base_min, base_max = master_price_range.get(masterCategory, (10, 100))\n",
        "\n",
        "    # ƒêi·ªÅu ch·ªânh theo subCategory\n",
        "    adj_min, adj_max = sub_price_adjustment.get(subCategory, (0, 0))\n",
        "\n",
        "    # ƒêi·ªÅu ch·ªânh theo articleType\n",
        "    art_min, art_max = article_price_range.get(articleType, (base_min + adj_min, base_max + adj_max))\n",
        "\n",
        "    # ƒêi·ªÅu ch·ªânh theo nƒÉm\n",
        "    if year < 2015:\n",
        "        discount = 0.3\n",
        "    elif year < 2018:\n",
        "        discount = 0.15\n",
        "    else:\n",
        "        discount = 0\n",
        "\n",
        "    final_min = art_min * (1 - discount)\n",
        "    final_max = art_max * (1 - discount)\n",
        "\n",
        "    return round(np.random.uniform(final_min, final_max) * 2500, 0)"
      ],
      "metadata": {
        "id": "x5NIe0IwwRkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_price_column(example):\n",
        "    example[\"price_in_vnd\"] = assign_price(\n",
        "        example[\"masterCategory\"],\n",
        "        example[\"subCategory\"],\n",
        "        example[\"articleType\"],\n",
        "        example[\"year\"],\n",
        "        example[\"season\"],\n",
        "        example[\"usage\"]\n",
        "    )\n",
        "    return example\n"
      ],
      "metadata": {
        "id": "Yc__y4hlwfT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_null(example):\n",
        "    # Ki·ªÉm tra xem c√≥ gi√° tr·ªã n√†o trong example b·ªã None kh√¥ng\n",
        "    return any(val is None for val in example.values())\n",
        "\n",
        "missing_null = train_dataset.filter(has_null)\n",
        "print(f\"S·ªë record c√≥ gi√° tr·ªã null: {len(missing_null)}\")"
      ],
      "metadata": {
        "id": "O3CR-YHywfr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict[\"train\"] = dataset_dict[\"train\"].map(add_price_column)\n",
        "# Split the training data into train and test (let's say 10% for the test set)\n",
        "train_test_split = dataset_dict['train'].train_test_split(test_size=0.1, seed = 42)\n",
        "# Further split the training set to get a validation set (e.g., 10% of the training set)\n",
        "train_val_split = train_test_split['train'].train_test_split(test_size=0.1, seed = 42)"
      ],
      "metadata": {
        "id": "842AO9WjwhxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the splits into a new DatasetDict\n",
        "final_dataset = {\n",
        "    'train': train_val_split['train'],\n",
        "    'val': train_val_split['test'],\n",
        "    'test': train_test_split['test']\n",
        "}\n",
        "\n",
        "final_dataset"
      ],
      "metadata": {
        "id": "qNCyNpVcwlD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = final_dataset[\"train\"]\n",
        "val_dataset = final_dataset[\"val\"]\n",
        "test_dataset = final_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "xHkgQOyzwn6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.features"
      ],
      "metadata": {
        "id": "DShZ-ckVwwKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.features import Image\n",
        "features = Features({\n",
        "    'id': Value('int64'),\n",
        "    'gender': Value('string'),\n",
        "    'masterCategory': Value('string'),\n",
        "    'subCategory': Value('string'),\n",
        "    'articleType': Value('string'),\n",
        "    'baseColour': Value('string'),\n",
        "    'season': Value('string'),\n",
        "    'year': Value('int64'),\n",
        "    'usage': Value('string'),\n",
        "    'productDisplayName': Value('string'),\n",
        "    'image': Image(),\n",
        "    'price_in_vnd': Value('int64')\n",
        "})\n"
      ],
      "metadata": {
        "id": "IzMpTlkZwrX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuy·ªÉn ƒë·ªïi dataset v·ªõi features m·ªõi\n",
        "train_dataset = train_dataset.cast(features)\n",
        "val_dataset = val_dataset.cast(features)\n",
        "test_dataset = test_dataset.cast(features)"
      ],
      "metadata": {
        "id": "2SU8bCW2wwu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Define Length in ProductDisplayName"
      ],
      "metadata": {
        "id": "1b0LytWZw12b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df= df.drop(columns= 'token_length')\n",
        "# df.to_csv(r'/home/ailab1/student_share/Thanh/GPU/Data/df_train.csv', index = False)"
      ],
      "metadata": {
        "id": "Qi0dFclMw_I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(r'/home/ailab1/student_share/Thanh/GPU/Data/df_train.csv')\n",
        "df['token_length'] = df['productDisplayName'].apply(lambda x: len(tokenizer.tokenize(x)))\n"
      ],
      "metadata": {
        "id": "TFgC_ffhw69F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ki·ªÉm tra ph√¢n ph·ªëi ƒë·ªô d√†i token\n",
        "print(df['token_length'].describe())\n",
        "# V·∫Ω histogram ƒë·ªô d√†i tokens\n",
        "plt.hist(df['token_length'], bins=30, color='blue', alpha=0.7)\n",
        "plt.xlabel(\"S·ªë l∆∞·ª£ng tokens\")\n",
        "plt.ylabel(\"S·ªë d√≤ng d·ªØ li·ªáu\")\n",
        "plt.title(\"Ph√¢n ph·ªëi ƒë·ªô d√†i token\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kddUIk4Lw9NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(tokenizer.tokenize(text)) for text in df[\"productDisplayName\"]]\n",
        "print(f\"ƒê·ªô d√†i trung b√¨nh: {np.mean(lengths)}\")\n",
        "print(f\"ƒê·ªô d√†i l·ªõn nh·∫•t: {np.max(lengths)}\")\n",
        "print(f\"90% m√¥ t·∫£ c√≥ ƒë·ªô d√†i d∆∞·ªõi: {np.percentile(lengths, 90)} tokens\")\n",
        "print(f\"95% m√¥ t·∫£ c√≥ ƒë·ªô d√†i d∆∞·ªõi: {np.percentile(lengths, 95)} tokens\")"
      ],
      "metadata": {
        "id": "NZZhq25YxBOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Label Encoder"
      ],
      "metadata": {
        "id": "tfyMpEzTxC3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_brand(name, gender):\n",
        "    # Chuy·ªÉn ƒë·ªïi sang ch·ªØ th∆∞·ªùng ƒë·ªÉ t√¨m ki·∫øm kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng\n",
        "    name_lower = name.lower()\n",
        "    gender_lower = gender.lower()\n",
        "    # T√°ch c√°c t·ª´ c·ªßa productDisplayName\n",
        "    tokens = name.split(\" \")\n",
        "    tokens_lower = name_lower.split(\" \")\n",
        "    if gender_lower in tokens_lower:\n",
        "        idx = tokens_lower.index(gender_lower)\n",
        "        brand_tokens = tokens[:idx]  # L·∫•y t·∫•t c·∫£ t·ª´ tr∆∞·ªõc t·ª´ gender\n",
        "        # T√πy ch·ªânh: N·∫øu mu·ªën lo·∫°i b·ªè t·ª´ \"by\" n·∫øu n·∫±m gi·ªØa, c√≥ th·ªÉ x·ª≠ l√Ω th√™m\n",
        "        brand = \" \".join(brand_tokens).replace(\"by\", \"\").strip()\n",
        "        return brand\n",
        "    else:\n",
        "        return tokens[0]"
      ],
      "metadata": {
        "id": "KqMy3mPpxE9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_brand_mapping(dataset):\n",
        "    # Duy·ªát qua t·ª´ng productDisplayName v√† s·ª≠ d·ª•ng c·ªôt gender t∆∞∆°ng ·ª©ng\n",
        "    brands = []\n",
        "    for name, gender in zip(dataset[\"productDisplayName\"], dataset[\"gender\"]):\n",
        "        brands.append(extract_brand(name, gender))\n",
        "    unique_brands = sorted(list(set(brands)))\n",
        "    mapping = {brand: idx for idx, brand in enumerate(unique_brands)}\n",
        "    inv_mapping = {idx: brand for brand, idx in mapping.items()}\n",
        "    return mapping, inv_mapping"
      ],
      "metadata": {
        "id": "UKyarf4vxG60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H√†m x√¢y d·ª±ng mapping cho m·ªôt c·ªôt\n",
        "def build_mapping(dataset, col):\n",
        "    unique_labels = dataset.unique(col)\n",
        "    unique_labels.sort()  # S·∫Øp x·∫øp ƒë·ªÉ nh·∫•t qu√°n (t√πy ch·ªçn)\n",
        "    mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    inv_mapping = {idx: label for label, idx in mapping.items()}\n",
        "    return mapping, inv_mapping"
      ],
      "metadata": {
        "id": "T9UUzUgRxIys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V√≠ d·ª• v·ªõi c√°c c·ªôt danh m·ª•c:\n",
        "gender_map, inv_gender_map = build_mapping(train_dataset_org, \"gender\")\n",
        "master_category_map, inv_master_category_map = build_mapping(train_dataset_org, \"masterCategory\")\n",
        "sub_category_map, inv_sub_category_map = build_mapping(train_dataset_org, \"subCategory\")\n",
        "article_type_map, inv_article_type_map = build_mapping(train_dataset_org, \"articleType\")\n",
        "base_colour_map, inv_base_colour_map = build_mapping(train_dataset_org, \"baseColour\")\n",
        "usage_map, inv_usage_map = build_mapping(train_dataset_org, \"usage\")\n",
        "brand_map, inv_brand_map = build_brand_mapping(train_dataset_org)\n",
        "\n",
        "# Ki·ªÉm tra c√°c mapping:\n",
        "print(\"Gender mapping:\", gender_map)\n",
        "print(\"Master Category mapping:\", master_category_map)\n",
        "print(\"Sub Category mapping:\", sub_category_map)\n",
        "print(\"Article Type mapping:\", article_type_map)\n",
        "print(\"Base Colour mapping:\", base_colour_map)\n",
        "print(\"Usage mapping:\", usage_map)\n",
        "print(\"Brand mapping:\", brand_map)"
      ],
      "metadata": {
        "id": "_EW1W64JxKbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_categorical(example):\n",
        "    # C·∫≠p nh·∫≠t c√°c c·ªôt theo mapping ƒë√£ x√¢y d·ª±ng\n",
        "\n",
        "    example[\"masterCategory\"] = master_category_map[example[\"masterCategory\"]]\n",
        "    example[\"subCategory\"] = sub_category_map[example[\"subCategory\"]]\n",
        "    example[\"articleType\"] = article_type_map[example[\"articleType\"]]\n",
        "    example[\"baseColour\"] = base_colour_map[example[\"baseColour\"]]\n",
        "    example[\"usage\"] = usage_map[example[\"usage\"]]\n",
        "\n",
        "    # N·∫øu dataset c·ªßa b·∫°n v·∫´n c√≥ c·ªôt \"productDisplayName\",\n",
        "    # ta c√≥ th·ªÉ tr√≠ch xu·∫•t brand t·ª´ ƒë√≥ v√† chuy·ªÉn sang gi√° tr·ªã s·ªë theo mapping:\n",
        "    extracted = extract_brand(example[\"productDisplayName\"], example[\"gender\"])\n",
        "    # L∆∞u √Ω: n·∫øu kh√¥ng t√¨m th·∫•y, extract_brand s·∫Ω tr·∫£ v·ªÅ token ƒë·∫ßu ti√™n\n",
        "    # v√† mapping brand c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ d·ªØ li·ªáu ban ƒë·∫ßu.\n",
        "    example[\"brand\"] = brand_map.get(extracted, -1)  # N·∫øu kh√¥ng c√≥, ƒë·∫∑t gi√° tr·ªã -1 (ho·∫∑c gi√° tr·ªã m·∫∑c ƒë·ªãnh)\n",
        "    example[\"gender\"] = gender_map[example[\"gender\"]]\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(update_categorical)\n",
        "val_dataset = val_dataset.map(update_categorical)\n",
        "test_dataset = test_dataset.map(update_categorical)"
      ],
      "metadata": {
        "id": "qJrrSXiFxQhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T·∫°o MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# Fit tr√™n train dataset tr∆∞·ªõc\n",
        "scaling_values_train = np.array([train_dataset[col] for col in [\"year\", \"price_in_vnd\"]]).T\n",
        "scaled_values_train = min_max_scaler.fit_transform(scaling_values_train)\n",
        "\n",
        "# √Åp d·ª•ng MinMaxScaler ƒë√£ fit cho val & test\n",
        "def scale_dataset(dataset, min_max_scaler):\n",
        "    scaling_values = np.array([dataset[col] for col in [\"year\", \"price_in_vnd\"]]).T\n",
        "    scaled_values = min_max_scaler.transform(scaling_values)  # Ch·ªâ transform, kh√¥ng fit l·∫°i\n",
        "    return dataset.map(lambda x, idx: {'year': scaled_values[idx][0], 'price_in_vnd': scaled_values[idx][1]}, with_indices=True)\n",
        "\n",
        "train_dataset = scale_dataset(train_dataset, min_max_scaler)\n",
        "val_dataset = scale_dataset(val_dataset, min_max_scaler)\n",
        "test_dataset = scale_dataset(test_dataset, min_max_scaler)"
      ],
      "metadata": {
        "id": "DDSTYz9DxU4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "season_mapping = {'Spring': 0, 'Summer': 1, 'Fall': 2, 'Winter': 3}\n",
        "\n",
        "train_dataset = train_dataset.map(lambda x: {'season': season_mapping[x['season']]})\n",
        "val_dataset = val_dataset.map(lambda x: {'season': season_mapping[x['season']]})\n",
        "test_dataset = test_dataset.map(lambda x: {'season': season_mapping[x['season']]})"
      ],
      "metadata": {
        "id": "Ie6ViRsfxYsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in [\"gender\", \"masterCategory\", \"subCategory\", \"articleType\", \"baseColour\", \"season\", \"usage\", \"brand\"]:\n",
        "    unique_values = train_dataset.unique(col)\n",
        "    print(f\"S·ªë l∆∞·ª£ng gi√° tr·ªã unique trong c·ªôt '{col}': {len(unique_values)}\")\n",
        "    print(f\"Gi√° tr·ªã unique: {unique_values}\")\n",
        "    print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "OhGsoH95xa7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 BERT Tokenizer for Transformer"
      ],
      "metadata": {
        "id": "xw_CN7W7xccc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"productDisplayName\"], padding=\"max_length\", truncation=True, max_length=13)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "# Lo·∫°i b·ªè c·ªôt productDisplayName sau khi tokenized\n",
        "train_dataset = train_dataset.remove_columns([\"productDisplayName\"])\n",
        "val_dataset = val_dataset.remove_columns([\"productDisplayName\"])\n",
        "test_dataset = test_dataset.remove_columns([\"productDisplayName\"])\n",
        "train_dataset.features"
      ],
      "metadata": {
        "id": "2AfixmzfxfPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Preprocess Image"
      ],
      "metadata": {
        "id": "No0MMf_2xkKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]\n",
        "import psutil\n",
        "\n",
        "# Dung l∆∞·ª£ng RAM kh·∫£ d·ª•ng v√† ƒë√£ s·ª≠ d·ª•ng\n",
        "total_memory = psutil.virtual_memory().total / (1024**3)  # GB\n",
        "used_memory = psutil.virtual_memory().used / (1024**3)  # GB\n",
        "available_memory = psutil.virtual_memory().available / (1024**3)  # GB\n",
        "\n",
        "print(f\"üñ•Ô∏è T·ªïng RAM: {total_memory:.2f} GB\")\n",
        "print(f\"üöÄ RAM ƒëang d√πng: {used_memory:.2f} GB\")\n",
        "print(f\"üîÑ RAM kh·∫£ d·ª•ng: {available_memory:.2f} GB\")"
      ],
      "metadata": {
        "id": "J5XR3lJMxiia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    gpu_memory_used = torch.cuda.memory_allocated(0) / (1024**3)\n",
        "    gpu_memory_free = gpu_memory_total - gpu_memory_used\n",
        "\n",
        "    print(f\"üñ•Ô∏è T·ªïng b·ªô nh·ªõ GPU: {gpu_memory_total:.2f} GB\")\n",
        "    print(f\"üöÄ GPU ƒëang d√πng: {gpu_memory_used:.2f} GB\")\n",
        "    print(f\"üîÑ GPU c√≤n tr·ªëng: {gpu_memory_free:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå Kh√¥ng c√≥ GPU!\")"
      ],
      "metadata": {
        "id": "SbY9MkKVxnBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "# Ki·ªÉm tra v√† ch·ªçn GPU n·∫øu c√≥\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load feature extractor c·ªßa ViT\n",
        "from transformers import ViTFeatureExtractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "# T·∫°o transform ch·∫°y tr√™n GPU\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "# H√†m x·ª≠ l√Ω ·∫£nh tr√™n GPU\n",
        "def preprocess_function(batch):\n",
        "    processed_images = []\n",
        "\n",
        "    for img in batch[\"image\"]:\n",
        "        if isinstance(img, str):  # N·∫øu l√† ƒë∆∞·ªùng d·∫´n, m·ªü ·∫£nh\n",
        "            pil_img = Image.open(img).convert(\"RGB\")\n",
        "        else:\n",
        "            pil_img = img.convert(\"RGB\")  # ƒê·∫£m b·∫£o l√† RGB\n",
        "\n",
        "        # Chuy·ªÉn ·∫£nh th√†nh tensor v√† ƒë·∫©y l√™n GPU\n",
        "        tensor_img = transform(pil_img).to(device)\n",
        "        processed_images.append(tensor_img)\n",
        "\n",
        "    # Chuy·ªÉn list th√†nh tensor batch tr√™n GPU\n",
        "    batch[\"pixel_values\"] = torch.stack(processed_images)\n",
        "\n",
        "    return batch\n",
        "\n",
        "# √Åp d·ª•ng preprocessing tr√™n GPU\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    num_proc=1,  # Gi·ªØ m·ª©c th·∫•p v√¨ ƒë√£ ch·∫°y tr√™n GPU\n",
        "    remove_columns=[\"image\"]\n",
        ").with_format(\"torch\")  # Chuy·ªÉn dataset sang ƒë·ªãnh d·∫°ng Torch"
      ],
      "metadata": {
        "id": "WmrS6Gesxpqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L·∫•y m·ªôt batch t·ª´ dataset ƒë·ªÉ ki·ªÉm tra\n",
        "example_batch = next(iter(train_dataset))\n",
        "\n",
        "# Chuy·ªÉn t·ª´ng ·∫£nh trong `pixel_values` th√†nh tensor\n",
        "processed_pixel_values = [torch.tensor(img, dtype=torch.float32) for img in example_batch[\"pixel_values\"]]\n",
        "\n",
        "# Chuy·ªÉn list th√†nh batch tensor\n",
        "pixel_values = torch.stack(processed_pixel_values)\n",
        "\n",
        "# Ki·ªÉm tra k·∫øt qu·∫£\n",
        "print(\"Pixel Values Shape:\", pixel_values.shape)  # üöÄ K·ª≥ v·ªçng: [batch_size, 3, 224, 224]\n",
        "print(\"Pixel Values dtype:\", pixel_values.dtype)  # üöÄ K·ª≥ v·ªçng: torch.float32\n"
      ],
      "metadata": {
        "id": "_RyxHQRrxzdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    num_proc=1,\n",
        "    remove_columns=[\"image\"]\n",
        ").with_format(\"torch\")"
      ],
      "metadata": {
        "id": "cBkDMXFByG0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    num_proc=1,\n",
        "    remove_columns=[\"image\"]\n",
        ").with_format(\"torch\")"
      ],
      "metadata": {
        "id": "BOrsc3TfyJQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III.  Fine-tuning Model (ViT)"
      ],
      "metadata": {
        "id": "54iQoMh8yLnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_ram = psutil.virtual_memory().available\n",
        "cpu_count = psutil.cpu_count(logical=True)\n",
        "print(f\"Available RAM: {available_ram / (1024**3):.2f} GB\")\n",
        "print(f\"CPU count: {cpu_count}\")"
      ],
      "metadata": {
        "id": "aS-gJtKPyM7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTModel\n",
        "# --- 1. Load m√¥ h√¨nh ViT pre-trained ·ªü ch·∫ø ƒë·ªô 8-bit ---\n",
        "# S·ª≠ d·ª•ng bitsandbytes ƒë·ªÉ t·∫£i model ·ªü ƒë·ªãnh d·∫°ng 8-bit nh·∫±m ti·∫øt ki·ªám b·ªô nh·ªõ GPU\n",
        "vit_model = ViTModel.from_pretrained(\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    # load_in_8bit=True,       # 8-bit quantization\n",
        "    device_map={\"\": 0}        # T·ª± ƒë·ªông ph√¢n b·ªï m√¥ h√¨nh tr√™n GPU\n",
        ")"
      ],
      "metadata": {
        "id": "zIPL4jhUyPvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 X√¢y d·ª±ng M√¥ H√¨nh ViT T√πy Ch·ªânh (Multi-Task Learning)"
      ],
      "metadata": {
        "id": "HO2GgsCGyRqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomViTHierarchical(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vit_model,\n",
        "                 num_gender=5,\n",
        "                 num_master=7,\n",
        "                 num_usage=8,\n",
        "                 num_sub=45,\n",
        "                 num_article=142,\n",
        "                 num_base=46,\n",
        "                 num_brand=820):\n",
        "        super(CustomViTHierarchical, self).__init__()\n",
        "        self.vit = vit_model\n",
        "        hidden_size = self.vit.config.hidden_size\n",
        "\n",
        "        # Level 1\n",
        "        self.head1_gender = nn.Linear(hidden_size, num_gender)\n",
        "        self.head1_master = nn.Linear(hidden_size, num_master)\n",
        "        self.head1_usage  = nn.Linear(hidden_size, num_usage)\n",
        "        self.level1_dim = num_gender + num_master + num_usage\n",
        "\n",
        "        # Level 2\n",
        "        self.head2 = nn.Sequential(\n",
        "            nn.Linear(hidden_size + self.level1_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_sub)\n",
        "        )\n",
        "        self.level2_dim = self.level1_dim + num_sub\n",
        "\n",
        "        # Level 3\n",
        "        self.head3 = nn.Sequential(\n",
        "            nn.Linear(hidden_size + self.level2_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_article)\n",
        "        )\n",
        "        self.level3_dim = self.level2_dim + num_article\n",
        "\n",
        "        # Level 4\n",
        "        self.head4 = nn.Sequential(\n",
        "            nn.Linear(hidden_size + self.level3_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_base + num_brand)\n",
        "        )\n",
        "        self.num_base = num_base\n",
        "        self.num_brand = num_brand\n",
        "\n",
        "    def forward(self, pixel_values, labels=None, **kwargs):\n",
        "        outputs = self.vit(pixel_values=pixel_values)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # (batch, hidden_size)\n",
        "\n",
        "        # Level 1\n",
        "        pred_gender = self.head1_gender(cls_output)\n",
        "        pred_master = self.head1_master(cls_output)\n",
        "        pred_usage  = self.head1_usage(cls_output)\n",
        "        level1_pred = torch.cat([pred_gender, pred_master, pred_usage], dim=1)\n",
        "\n",
        "        # Level 2\n",
        "        level2_input = torch.cat([cls_output, level1_pred], dim=1)\n",
        "        pred_sub = self.head2(level2_input)\n",
        "        level2_pred = torch.cat([level1_pred, pred_sub], dim=1)\n",
        "\n",
        "        # Level 3\n",
        "        level3_input = torch.cat([cls_output, level2_pred], dim=1)\n",
        "        pred_article = self.head3(level3_input)\n",
        "        level3_pred = torch.cat([level2_pred, pred_article], dim=1)\n",
        "\n",
        "        # Level 4\n",
        "        level4_input = torch.cat([cls_output, level3_pred], dim=1)\n",
        "        pred_base_brand = self.head4(level4_input)\n",
        "        pred_base = pred_base_brand[:, :self.num_base]\n",
        "        pred_brand = pred_base_brand[:, self.num_base:]\n",
        "\n",
        "        outputs_dict = {\n",
        "            \"gender\": pred_gender,\n",
        "            \"master\": pred_master,\n",
        "            \"usage\": pred_usage,\n",
        "            \"sub\": pred_sub,\n",
        "            \"article\": pred_article,\n",
        "            \"base\": pred_base,\n",
        "            \"brand\": pred_brand\n",
        "        }\n",
        "\n",
        "        # N·∫øu labels ƒë∆∞·ª£c cung c·∫•p, t√≠nh loss cho t·ª´ng task v√† c·ªông d·ªìn\n",
        "        if labels is not None:\n",
        "            loss = 0\n",
        "            for key in labels:\n",
        "                # Gi·∫£ s·ª≠ c√°c nh√£n trong labels c√≥ c√πng c√°c key: \"gender\", \"master\", ...\n",
        "                loss += F.cross_entropy(outputs_dict[key], labels[key])\n",
        "            outputs_dict[\"loss\"] = loss\n",
        "\n",
        "        return outputs_dict"
      ],
      "metadata": {
        "id": "cPFLf5s0yTzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomViTHierarchical(vit_model)\n",
        "# Freeze t·∫•t c·∫£ tham s·ªë trong backbone ViT\n",
        "for param in vit_model.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "8iwD6cN0yWWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Danh s√°ch c√°c module ch·ª©a 'attn' ho·∫∑c 'attention':\")\n",
        "for name, module in model.named_modules():\n",
        "    if 'attn' in name or 'attention' in name:\n",
        "        print(name)\n",
        "        for sub_name, sub_module in module.named_modules():\n",
        "            print(f\"  - {sub_name}\")\n",
        "from peft import LoraConfig, get_peft_model\n"
      ],
      "metadata": {
        "id": "NWz0pLrFyYrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. C·∫•u h√¨nh v√† ch√®n LoRA v√†o m√¥ h√¨nh ---\n",
        "# Ch√∫ng ta √°p d·ª•ng LoRA v√†o c√°c projection layers c·ªßa ph·∫ßn attention trong backbone ViT.\n",
        "# V√¨ model c·ªßa b·∫°n c√≥ ViT ƒë∆∞·ª£c l∆∞u trong attribute 'vit', n√™n t√™n c√°c module s·∫Ω c√≥ ti·ªÅn t·ªë 'vit.'.\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                # Rank c·ªßa ma tr·∫≠n low-rank\n",
        "    lora_alpha=32,       # H·ªá s·ªë scaling cho LoRA\n",
        "    target_modules=[\n",
        "        \"query\",\n",
        "        \"key\",\n",
        "        \"value\",\n",
        "        \"dense\"\n",
        "    ],\n",
        "    lora_dropout=0.05,   # T·ªâ l·ªá dropout cho LoRA\n",
        "    bias=\"none\",         # Kh√¥ng c·∫≠p nh·∫≠t bias\n",
        "    task_type=\"FEATURE_EXTRACTION\"  # Lo·∫°i nhi·ªám v·ª• (ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn)\n",
        ")\n",
        "# Ch√®n LoRA v√†o m√¥ h√¨nh (freeze h·∫ßu h·∫øt tham s·ªë, ch·ªâ c·∫≠p nh·∫≠t c√°c tham s·ªë LoRA)\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "F_lJwE6oyah5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. In s·ªë l∆∞·ª£ng tham s·ªë trainable ƒë·ªÉ x√°c nh·∫≠n ---\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    total_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        total_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params} || Total params: {total_params} || Trainable%: {100 * trainable_params / total_params:.2f}%\")\n",
        "\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "llvAbthIyfa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    metrics = {}\n",
        "    total_loss = 0.0\n",
        "    num_tasks = 0\n",
        "\n",
        "    # Gi·∫£ s·ª≠ c√°c task c√≥ c√°c key: \"gender\", \"master\", \"usage\", \"sub\", \"article\", \"base\", \"brand\"\n",
        "    for key in labels:\n",
        "        # Chuy·ªÉn ƒë·ªïi n·∫øu c·∫ßn (ƒë·∫£m b·∫£o l√† tensor)\n",
        "        task_logits = torch.tensor(logits[key]) if not torch.is_tensor(logits[key]) else logits[key]\n",
        "        task_labels = torch.tensor(labels[key]) if not torch.is_tensor(labels[key]) else labels[key]\n",
        "\n",
        "        # T√≠nh loss cho task hi·ªán t·∫°i\n",
        "        loss_val = F.cross_entropy(task_logits, task_labels, reduction=\"mean\")\n",
        "        total_loss += loss_val.item()\n",
        "        num_tasks += 1\n",
        "\n",
        "        # T√≠nh accuracy v√† f1\n",
        "        pred_ids = torch.argmax(task_logits, dim=-1).cpu().numpy()\n",
        "        true_ids = task_labels.cpu().numpy()\n",
        "        acc = accuracy_score(true_ids, pred_ids)\n",
        "        f1 = f1_score(true_ids, pred_ids, average='weighted')\n",
        "\n",
        "        metrics[f\"{key}_accuracy\"] = acc\n",
        "        metrics[f\"{key}_f1\"] = f1\n",
        "\n",
        "    # T√≠nh loss trung b√¨nh cho t·∫•t c·∫£ c√°c task v√† tr·∫£ v·ªÅ d∆∞·ªõi key \"eval_loss\"\n",
        "    metrics[\"eval_loss\"] = total_loss / num_tasks if num_tasks > 0 else 0.0\n",
        "\n",
        "    # T√≠nh trung b√¨nh c√°c ch·ªâ s·ªë accuracy v√† f1 n·∫øu c·∫ßn\n",
        "    acc_values = [v for k, v in metrics.items() if k.endswith('_accuracy')]\n",
        "    f1_values = [v for k, v in metrics.items() if k.endswith('_f1')]\n",
        "    metrics[\"eval_average_accuracy\"] = sum(acc_values) / len(acc_values) if acc_values else 0.0\n",
        "    metrics[\"eval_average_f1\"] = sum(f1_values) / len(f1_values) if f1_values else 0.0\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "oNwekVgcyhXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_dataset = test_dataset.select(range(10))\n",
        "predictions = trainer.predict(small_test_dataset)\n",
        "print(\"Predictions:\", predictions.metrics)"
      ],
      "metadata": {
        "id": "yJw7YOiMyn9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_eval_dataset = val_dataset.select(range(10))  # ch·ªçn 10 m·∫´u ƒë·∫ßu ti√™n\n",
        "eval_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)\n",
        "print(\"Eval metrics:\", eval_metrics)"
      ],
      "metadata": {
        "id": "YXhR7o4zylvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import Trainer\n",
        "\n",
        "def custom_compute_loss(model, inputs, num_items_in_batch):\n",
        "    # L·∫•y nh√£n t·ª´ inputs\n",
        "    labels = {\n",
        "        \"gender\": inputs[\"gender\"],\n",
        "        \"master\": inputs[\"master\"],\n",
        "        \"usage\": inputs[\"usage\"],\n",
        "        \"sub\": inputs[\"sub\"],\n",
        "        \"article\": inputs[\"article\"],\n",
        "        \"base\": inputs[\"base\"],\n",
        "        \"brand\": inputs[\"brand\"],\n",
        "    }\n",
        "\n",
        "    # G·ªçi model v·ªõi c√°c inputs c·∫ßn thi·∫øt\n",
        "    outputs = model(\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        token_type_ids=inputs[\"token_type_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"]\n",
        "    )\n",
        "\n",
        "    # T√≠nh loss cho t·ª´ng task (s·ª≠ d·ª•ng CrossEntropyLoss)\n",
        "    loss = 0\n",
        "    for key in labels:\n",
        "        loss += F.cross_entropy(outputs[key], labels[key])\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "74YPcD0Myo70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def data_collator(features):\n",
        "    # V√¨ c√°c tr∆∞·ªùng n√†y ƒë√£ ·ªü d·∫°ng tensor, n√™n d√πng torch.stack() thay v√¨ torch.tensor()\n",
        "    input_ids = torch.stack([f[\"input_ids\"] for f in features])\n",
        "    token_type_ids = torch.stack([f[\"token_type_ids\"] for f in features])\n",
        "    attention_mask = torch.stack([f[\"attention_mask\"] for f in features])\n",
        "\n",
        "    # Gh√©p batch c·ªßa ·∫£nh\n",
        "    pixel_values = torch.stack([f[\"pixel_values\"].clone().detach().float() for f in features])\n",
        "\n",
        "    # C√°c nh√£n th∆∞·ªùng l√† ki·ªÉu s·ªë nguy√™n n√™n d√πng torch.tensor() l√† ·ªïn\n",
        "    labels = {\n",
        "        \"gender\": torch.tensor([f[\"gender\"] for f in features], dtype=torch.long),\n",
        "        \"master\": torch.tensor([f[\"masterCategory\"] for f in features], dtype=torch.long),\n",
        "        \"usage\": torch.tensor([f[\"usage\"] for f in features], dtype=torch.long),\n",
        "        \"sub\": torch.tensor([f[\"subCategory\"] for f in features], dtype=torch.long),\n",
        "        \"article\": torch.tensor([f[\"articleType\"] for f in features], dtype=torch.long),\n",
        "        \"base\": torch.tensor([f[\"baseColour\"] for f in features], dtype=torch.long),\n",
        "        \"brand\": torch.tensor([f[\"brand\"] for f in features], dtype=torch.long),\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"token_type_ids\": token_type_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        **labels,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "5NOEvtdlyrGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L·∫•y m·ªôt s·ªë m·∫´u t·ª´ train_dataset (v√≠ d·ª• 5 m·∫´u)\n",
        "sample_features = [train_dataset[i] for i in range(5)]"
      ],
      "metadata": {
        "id": "gXM6yDOmytmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# G·ªçi h√†m data_collator tr√™n c√°c m·∫´u ƒë√≥\n",
        "batch = data_collator(sample_features)"
      ],
      "metadata": {
        "id": "jdnR8AbJyu1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In th√¥ng tin c·ªßa t·ª´ng key trong batch\n",
        "print(\"C√°c key c√≥ trong batch:\", list(batch.keys()))\n",
        "print(\"Pixel Values shape:\", batch[\"pixel_values\"].shape)\n",
        "print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
        "print(\"Token Type IDs shape:\", batch[\"token_type_ids\"].shape)\n",
        "print(\"Attention Mask shape:\", batch[\"attention_mask\"].shape)"
      ],
      "metadata": {
        "id": "TmXUzDxSywJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In shape c·ªßa c√°c nh√£n\n",
        "for label_key in [\"gender\", \"master\", \"usage\", \"sub\", \"article\", \"base\", \"brand\"]:\n",
        "    print(f\"{label_key} shape:\", batch[label_key].shape)\n"
      ],
      "metadata": {
        "id": "VjBzHh9fyx5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit_hierarchical\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1,  # L∆∞u m·ªói epoch thay v√¨ m·ªói 500 step\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,  # ‚ö°Ô∏è Mixed Precision (TƒÉng t·ªëc GPU)\n",
        "    load_best_model_at_end=True,\n",
        "    dataloader_num_workers=0,  # üî• T·∫Øt multi-processing tr√°nh l·ªói CUDA fork\n",
        "    # metric_for_best_model=\"average_f1\",\n",
        "    remove_unused_columns=False,  # ‚ö†Ô∏è Quan tr·ªçng: tr√°nh l·ªói khi c√≥ nhi·ªÅu feature\n",
        "    logging_first_step=True,\n",
        "    dataloader_pin_memory=False,\n",
        ")"
      ],
      "metadata": {
        "id": "_aLWdTgay0Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # L·∫•y num_items_in_batch t·ª´ kwargs n·∫øu c√≥, ng∆∞·ª£c l·∫°i l·∫•y batch size\n",
        "        num_items_in_batch = kwargs.get(\"num_items_in_batch\", len(inputs[\"input_ids\"]))\n",
        "        # T√≠nh loss b·∫±ng h√†m custom_compute_loss m√† b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a\n",
        "        loss = custom_compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
        "        if return_outputs:\n",
        "            return loss, None\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "epLE4hmty2l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "# model_name = \"google/vit-large-patch16-224\"\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-large-patch16-224\")\n",
        "processor\n",
        "# Kh·ªüi t·∫°o CustomTrainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "KdQIQdMjy4RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "UvgX50B-y6pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = trainer.predict(test_dataset)\n",
        "print(outputs.metrics)"
      ],
      "metadata": {
        "id": "an2Rfb6My792"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L∆∞u model v√† c·∫•u h√¨nh\n",
        "model.save_pretrained(\"./my_finetuned_vit\")"
      ],
      "metadata": {
        "id": "qcKyGxoLy9f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N·∫øu c√≥ tokenizer (v√≠ d·ª•: BERT tokenizer)\n",
        "tokenizer.save_pretrained(\"./my_finetuned_vit\")\n",
        "\n",
        "from transformers import ViTModel, BertTokenizer\n",
        "\n",
        "model = ViTModel.from_pretrained(\"./my_finetuned_vit\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./my_finetuned_vit\")\n"
      ],
      "metadata": {
        "id": "7fi6X1_Uy_B9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}